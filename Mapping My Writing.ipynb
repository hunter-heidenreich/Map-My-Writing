{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import library\n",
    "import graphs\n",
    "\n",
    "import analysis.basics as basics\n",
    "\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the document directories\n",
    "dirs = ['/Users/hunterheidenreich/git/MapMyWriting/data/facebook_chat',\n",
    "       '/Users/hunterheidenreich/git/MapMyWriting/data/facebook_comments',\n",
    "       '/Users/hunterheidenreich/git/MapMyWriting/data/facebook_posts',\n",
    "       '/Users/hunterheidenreich/git/MapMyWriting/data/journals',\n",
    "       '/Users/hunterheidenreich/git/MapMyWriting/data/school',\n",
    "       '/Users/hunterheidenreich/git/MapMyWriting/data/songs']\n",
    "\n",
    "input_files = []\n",
    "for text_dir in dirs:\n",
    "    input_files += library.get_file_list(text_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load into a collection of texts\n",
    "text_collection = {}\n",
    "\n",
    "for f in input_files:\n",
    "    base = os.path.basename(f)\n",
    "    \n",
    "    if base not in text_collection:\n",
    "        text_collection[base] = library.FileObject()\n",
    "    \n",
    "    with open(f, 'r') as in_file:\n",
    "        text_collection[base]._lines += in_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text_collection.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do pre-processing for the texts\n",
    "\n",
    "for key, value in text_collection.items():\n",
    "    raw, stop, stem, lemma = library.preprocess_text(value.lines)\n",
    "    text_collection[key].raw = raw\n",
    "    text_collection[key].stop = stop\n",
    "    text_collection[key].stem = stem\n",
    "    text_collection[key].lemma = lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = []\n",
    "for key, value in text_collection.items():\n",
    "    lemmas.append(value.lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raws = []\n",
    "names = []\n",
    "for key, value in text_collection.items():\n",
    "    raws.append(value.raw)\n",
    "    names.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at word usage graphically \n",
    "counter = basics.VocabUtils.aggregate_words_counts(lemmas)\n",
    "word_counts_raw = list(counter.values())\n",
    "word_counts_sorted = sorted(word_counts_raw)\n",
    "\n",
    "cap = min(len(word_counts_raw), 100)\n",
    "\n",
    "graphs.plot_bar_graph(range(len(word_counts_raw[:cap])), word_counts_raw[:cap], \n",
    "                      x_label='Words', y_label='Counts', title='Word Frequencies (Raw)',\n",
    "                      export=True, export_name='visualizations/word_freq_bar_raw.png')\n",
    "graphs.plot_bar_graph(range(len(word_counts_sorted[:cap])), word_counts_sorted[-cap:], \n",
    "                      x_label='Words', y_label='Counts', title='Word Frequencies (Sorted)',\n",
    "                      export=True, export_name='visualizations/word_freq_bar_sorted.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking some basic statistics for vocab usage \n",
    "counter = basics.VocabUtils.aggregate_words_counts(lemmas)\n",
    "data = [[len(input_files)],\n",
    "        [basics.VocabUtils.global_top_k_words(lemmas, k=1)[0][0] + ' (' + str(basics.VocabUtils.global_top_k_words(lemmas, k=1)[0][1]) + ')'], \n",
    "        [basics.VocabUtils.global_top_k_words(lemmas, k=len(list(counter.keys())))[-1][0] + ' (' + str(basics.VocabUtils.global_top_k_words(lemmas, k=len(list(counter.keys())))[-1][1]) + ')'],\n",
    "        [len(counter.items())],\n",
    "        [np.mean(list(counter.values()))],\n",
    "        [sum([len(word_list) for word_list in raws])],\n",
    "        [sum([sum([len(w) for w in word_list]) for word_list in raws])]]\n",
    "row_labels = ['Collection size: ', 'Top word: ', 'Least common: ', 'Vocab size: ', 'Average word usage count: ',\n",
    "              'Total words: ', 'Total characters: ']\n",
    "\n",
    "graphs.plot_table(cell_data=data, row_labels=row_labels,\n",
    "                  export=True, export_name='visualizations/basic_stats_table.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at word count over time\n",
    "raw_wc = [len(word_list) for word_list in raws]\n",
    "labels = library.get_datetimes(names)\n",
    "\n",
    "labels, raw_wc = zip(*sorted(zip(labels, raw_wc)))\n",
    "\n",
    "graphs.plot(labels, raw_wc, \n",
    "            x_label='Date', y_label='Word Count', title='Word Count Over Time',\n",
    "            export=True, export_name='visualizations/word_count_by_time.png')\n",
    "\n",
    "raw_wc_u = [len(list(basics.VocabUtils.unique_vocab(word_list).items())) for word_list in raws]\n",
    "graphs.plot(labels, raw_wc_u, \n",
    "            x_label='Date', y_label='Word Count', title='Unique Word Count Over Time',\n",
    "            export=True, export_name='visualizations/unique_word_count_by_time.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentiment import SentimentAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = SentimentAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = s.embed_sentence('This is a test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "a0 = np.mean(a[0], axis=0)\n",
    "a1 = np.mean(a[1], axis=0)\n",
    "a2 = np.mean(a[2], axis=0)\n",
    "red = (a0 + a1 + a2) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
